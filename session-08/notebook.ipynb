{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8: Image Story Pipeline\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/level-2-course-material/blob/main/session-08/notebook.ipynb)\n",
    "\n",
    "Chain two models together: image in, caption out, mood judged."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup — run this cell first! (this may take a minute)\n",
    "!pip install -q transformers torch Pillow\n",
    "\n",
    "from transformers import pipeline, BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(\"Loading BLIP captioner (this is a big model, be patient)...\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "print(\"BLIP loaded!\")\n",
    "\n",
    "print(\"Loading sentiment model...\")\n",
    "sentiment = pipeline(\"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "print(\"All models loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Built Tonight\n",
    "\n",
    "We built an **Image Story Pipeline** — two models chained together:\n",
    "1. **BLIP** looks at an image and writes a caption\n",
    "2. **DistilBERT** reads that caption and judges the mood\n",
    "\n",
    "Neither model knows the other exists.\n",
    "\n",
    "Check out the live Space: [Image Story Pipeline on Hugging Face](https://huggingface.co/spaces/profplate/image-story-pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The pipeline function: image -> caption -> sentiment\n",
    "def analyze_image(image):\n",
    "    # Step 1: Generate caption\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = caption_model.generate(**inputs, max_length=50)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # Step 2: Analyze caption sentiment\n",
    "    result = sentiment(caption)[0]\n",
    "\n",
    "    # Show the full pipeline\n",
    "    print(f\"Caption: {caption}\")\n",
    "    print(f\"Sentiment: {result['label']} ({result['score']:.1%})\")\n",
    "    print(f\"\\nFull pipeline:\")\n",
    "    print(f\"  IMAGE -> BLIP -> \\\"{caption}\\\"\")\n",
    "    print(f\"  \\\"{caption}\\\" -> DistilBERT -> {result['label']} ({result['score']:.1%})\")\n",
    "    return caption, result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Upload Images in Colab\n",
    "\n",
    "Run the cell below — it will open a **file picker**. Choose an image from your computer.\n",
    "\n",
    "This is a new Colab skill: working with files!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Upload an image from your computer\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Click 'Choose Files' to upload an image...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Open the uploaded image\n",
    "filename = list(uploaded.keys())[0]\n",
    "image = Image.open(io.BytesIO(uploaded[filename]))\n",
    "print(f\"\\nUploaded: {filename}\")\n",
    "image  # Display the image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the pipeline on your uploaded image\n",
    "caption, result = analyze_image(image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a URL Image (No Upload Needed)\n",
    "\n",
    "You can also load images from the web. This is handy for quick tests."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load an image from a URL\n",
    "import requests\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\"\n",
    "response = requests.get(url)\n",
    "web_image = Image.open(io.BytesIO(response.content))\n",
    "\n",
    "print(\"Image from the web:\")\n",
    "display(web_image)\n",
    "print()\n",
    "analyze_image(web_image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experiment 1: Find an Image Where the Caption Is Wrong\n",
    "\n",
    "Upload different images. Find one where BLIP describes it incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 1: Upload another image\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload an image that might confuse the captioner...\")\n",
    "uploaded2 = files.upload()\n",
    "filename2 = list(uploaded2.keys())[0]\n",
    "image2 = Image.open(io.BytesIO(uploaded2[filename2]))\n",
    "display(image2)\n",
    "print()\n",
    "analyze_image(image2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Caption Right, Sentiment Wrong?\n",
    "\n",
    "Find an image where the caption is accurate but the sentiment model gets the mood wrong."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 2: Upload another image\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload an image where the mood might be tricky...\")\n",
    "uploaded3 = files.upload()\n",
    "filename3 = list(uploaded3.keys())[0]\n",
    "image3 = Image.open(io.BytesIO(uploaded3[filename3]))\n",
    "display(image3)\n",
    "print()\n",
    "analyze_image(image3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Where Does the Error Start?\n",
    "\n",
    "For each image you tested, fill in this table:\n",
    "\n",
    "| Image | What It Actually Shows | BLIP Caption | Caption Correct? | Sentiment | Sentiment Correct? | Which Step Failed? |\n",
    "|-------|----------------------|-------------|-----------------|-----------|-------------------|-------------------|\n",
    "| Image 1 | | | | | | |\n",
    "| Image 2 | | | | | | |\n",
    "| Image 3 | | | | | | |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 3: Test the sentiment model directly on a corrected caption\n",
    "# If the caption was wrong, what SHOULD it have said?\n",
    "\n",
    "wrong_caption = \"a dog sitting on a couch\"  # <-- What BLIP actually said\n",
    "correct_caption = \"a cat sleeping on a bed\"  # <-- What it SHOULD have said\n",
    "\n",
    "print(\"Wrong caption:\")\n",
    "r1 = sentiment(wrong_caption)[0]\n",
    "print(f\"  \\\"{wrong_caption}\\\" -> {r1['label']} ({r1['score']:.1%})\")\n",
    "\n",
    "print(\"\\nCorrected caption:\")\n",
    "r2 = sentiment(correct_caption)[0]\n",
    "print(f\"  \\\"{correct_caption}\\\" -> {r2['label']} ({r2['score']:.1%})\")\n",
    "\n",
    "print(\"\\nDoes fixing the caption change the sentiment?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Find an image that **breaks the pipeline**. Figure out which step failed:\n",
    "- Did the captioner describe it incorrectly? (Step 1 error)\n",
    "- Did the sentiment model misread a correct caption? (Step 2 error)\n",
    "- Or did both steps fail?\n",
    "\n",
    "Bring your most interesting broken result to next session.\n",
    "\n",
    "**GitHub:** If you haven't uploaded a notebook yet, try it this week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|---------|\n",
    "| **Pipeline (multi-model)** | Connecting models so the output of one feeds the input of the next |\n",
    "| **Error cascade** | When one model's mistake causes every model after it to be wrong |\n",
    "| **Captioning** | Generating a text description of an image |\n",
    "| **Chain** | Linking multiple steps together where each depends on the previous one |\n",
    "| **Image-to-text** | A model that takes an image as input and produces text as output |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}