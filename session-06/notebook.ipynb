{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Domain Safari\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/level-2-course-material/blob/main/session-06/notebook.ipynb)\n",
    "\n",
    "Same models, different worlds of text. Watch them struggle."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup — run this cell first!\n",
    "!pip install -q transformers torch\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Loading 3 sentiment models...\")\n",
    "movie_model = pipeline(\"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "twitter_model = pipeline(\"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "review_model = pipeline(\"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "print(\"All 3 models loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Built Tonight\n",
    "\n",
    "No new Space tonight — we reused the **Sentiment Showdown** from Session 4.\n",
    "\n",
    "We tested these 3 models on text they were **never trained on** and watched them struggle.\n",
    "\n",
    "Check out the live Space: [Sentiment Showdown on Hugging Face](https://huggingface.co/spaces/profplate/sentiment-showdown)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper function: run text through all 3 models\n",
    "def compare_models(text):\n",
    "    results = {\n",
    "        \"Movie Model\": movie_model(text[:512])[0],\n",
    "        \"Twitter Model\": twitter_model(text[:512])[0],\n",
    "        \"Review Model\": review_model(text[:512])[0],\n",
    "    }\n",
    "    for name, r in results.items():\n",
    "        print(f\"  {name}: {r['label']} ({r['score']:.1%})\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 1: News Articles\n",
    "\n",
    "News text is neutral/factual. The movie model has to pick POSITIVE or NEGATIVE — it has no neutral option."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "news = \"The Federal Reserve announced a quarter-point interest rate cut on Wednesday, signaling confidence that inflation is moving sustainably toward its 2 percent target.\"\n",
    "print(\"NEWS:\")\n",
    "compare_models(news)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 2: Tweets\n",
    "\n",
    "Which model handles slang best? Has the product review model ever seen this kind of language?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tweet = \"ngl this new update is mid at best. they really thought they did something\"\n",
    "print(\"TWEET:\")\n",
    "compare_models(tweet)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 3: Song Lyrics\n",
    "\n",
    "Sounds upbeat (dancing, laughing, celebrating) but it's about destruction. Can the models tell?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lyrics = \"Dancing on the ceiling, burning down the walls, laughing at the wreckage as the empire falls. We'll celebrate the ending with confetti made of ash.\"\n",
    "print(\"SONG LYRICS:\")\n",
    "compare_models(lyrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 4: Student Essay\n",
    "\n",
    "Academic writing is carefully balanced — not really positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "essay = \"In conclusion, while both authors present compelling arguments, Smith's analysis is more thoroughly supported by evidence. However, Jones raises important counterpoints that cannot be ignored.\"\n",
    "print(\"STUDENT ESSAY:\")\n",
    "compare_models(essay)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 5: Text Messages\n",
    "\n",
    "Is \"lol ok sure whatever\" positive, negative, or sarcastic? Humans would need context."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "texts = [\n",
    "    \"lol ok sure whatever u say\",\n",
    "    \"omg YES that's literally the best thing ever im so happy rn\",\n",
    "]\n",
    "for t in texts:\n",
    "    print(f\"TEXT MESSAGE: {t}\")\n",
    "    compare_models(t)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 6: Legal Text\n",
    "\n",
    "Legal text has no sentiment — it's purely functional. But the models HAVE to output something."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "legal = \"The party of the first part shall indemnify and hold harmless the party of the second part against any and all claims, damages, losses, costs, and expenses arising out of or relating to any breach of this agreement.\"\n",
    "print(\"LEGAL TEXT:\")\n",
    "compare_models(legal)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Experiment 1: Test Your Own Domain\n",
    "\n",
    "Pick a type of text and run it through all 3 models. Record what happens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 1: Paste your own text here\n",
    "my_text = \"\"  # <-- Put your text inside the quotes\n",
    "\n",
    "if my_text:\n",
    "    print(\"MY TEXT:\")\n",
    "    compare_models(my_text)\n",
    "else:\n",
    "    print(\"Paste some text between the quotes above and run this cell again!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Find the Domain That Breaks All Models\n",
    "\n",
    "Try to find text where ALL 3 models get confused or give wrong answers.\n",
    "\n",
    "Ideas: mixing languages, heavy sarcasm, recipes, math problems, meme text, code comments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 2: Try to break all 3 models\n",
    "breaking_text = \"\"  # <-- Put your text here\n",
    "\n",
    "if breaking_text:\n",
    "    print(\"BREAKING TEXT:\")\n",
    "    compare_models(breaking_text)\n",
    "else:\n",
    "    print(\"Paste some text that you think will confuse the models!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Which Model Is Most Consistent?\n",
    "\n",
    "Run 3 different domains through all models. Which model gives the most reasonable answers across all of them?\n",
    "\n",
    "Record your observations here:\n",
    "\n",
    "| Domain | Movie Model | Twitter Model | Review Model | Which was best? |\n",
    "|--------|------------|---------------|--------------|----------------|\n",
    "| | | | | |\n",
    "| | | | | |\n",
    "| | | | | |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 3: Test 3 domains in a row\n",
    "domains = {\n",
    "    \"Poetry\": \"I wandered lonely through the ash of things that used to gleam. The world had shed its golden mask and left a hollow dream.\",\n",
    "    \"Code comment\": \"// HACK: This is a terrible workaround for the race condition. TODO: fix this properly before it breaks production.\",\n",
    "    \"Meme\": \"Nobody: Absolutely nobody: My cat at 3am: knocks everything off the counter\",\n",
    "}\n",
    "\n",
    "for domain, text in domains.items():\n",
    "    print(f\"{domain.upper()}:\")\n",
    "    compare_models(text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Observations\n",
    "\n",
    "Write your observations in this cell (double-click to edit):\n",
    "\n",
    "- Which model worked best across the most domains?\n",
    "- Were there domains where ALL models struggled?\n",
    "- What makes a domain \"hard\" for these models?\n",
    "\n",
    "*Your answers here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Find a domain that **breaks all three models**. Write down:\n",
    "1. The text you used\n",
    "2. What each model predicted\n",
    "3. Why you think it confused them\n",
    "\n",
    "**GitHub:** Write a README.md for your `my-ai-portfolio` repo this week!\n",
    "1. Go to your repo on github.com\n",
    "2. Click the pencil icon on README.md (or create one if it's missing)\n",
    "3. Write a few sentences about what you're learning in this course\n",
    "4. Add links to any notebooks you've uploaded\n",
    "5. Commit the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|---------|\n",
    "| **Domain** | A category of text (tweets, legal documents, poetry, product reviews, etc.) |\n",
    "| **Overfitting** | When a model is too specialized in its training data to handle new situations |\n",
    "| **Domain shift** | When the real-world data is different from the training data |\n",
    "| **Generalization** | A model's ability to work well on data it hasn't seen before |\n",
    "| **Distribution** | The patterns and characteristics of a particular type of text |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}